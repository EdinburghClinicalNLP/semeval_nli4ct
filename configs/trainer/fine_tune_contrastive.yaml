defaults:
  - base_trainer_config

# Exact same as fine tune, except this used contrastive samples
name: fine_tune_contrastive
configs:
  epochs: 10
  gradient_accumulation_steps: 32
  max_train_seq_len: 2048  # The maximum seq length to use for training in 1x80GB GPU
  fusion_strategy: ""
  explicit_evidence: False
  multi_adapter: False
  lora_configs:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.0
    target_modules: ["k_proj", "q_proj", "v_proj"]
