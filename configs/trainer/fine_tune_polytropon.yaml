defaults:
  - base_trainer_config

name: fine_tune_multi_adapter
configs:
  epochs: 10
  gradient_accumulation_steps: 32
  max_train_seq_len: 2048  # The maximum seq length to use for training in 1x80GB GPU
  fusion_strategy: ""
  explicit_evidence: False
  common_lora_config: null
  section_lora_config: null
  common_polytropon_config: null
  common_polytropon_config:
    r: 16
    n_tasks: 4
    n_skills: 2
    n_splits: 2
    
